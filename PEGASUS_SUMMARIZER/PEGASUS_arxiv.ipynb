{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29232896",
   "metadata": {},
   "source": [
    "## PEGASUS Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be9b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from parser.parser import Parser\n",
    "from Scorer import Score\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = 'google/pegasus-large'\n",
    "preTrainedModel = '../preTrainedModel/'\n",
    "torch_device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401b3c4",
   "metadata": {},
   "source": [
    "### Initializing the tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1da5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(preTrainedModel).to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b291f",
   "metadata": {},
   "source": [
    "### Function For Getting response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5418ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input_text):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=1024, return_tensors=\"pt\").to(torch_device)\n",
    "  gen_out = model.generate(**batch,max_length=128,num_beams=5, num_return_sequences=1, do_sample = True, temperature=1.5)\n",
    "  output_text = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n",
    "  return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a82718",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1ec634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.DataLoader import DataLoader\n",
    "\n",
    "#Loading the datasets through the custom dataloader.\n",
    "datasetLoader = DataLoader(datasetName='arxiv')\n",
    "arxiv_test = datasetLoader.getData('../Datasets/', split='test')\n",
    "# datasetLoader.datasetName = 'pubmed'\n",
    "# pubmed_test = datasetLoader.getData('../Datasets/', split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67fd189",
   "metadata": {},
   "source": [
    "## Running a sample instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b6c7ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Summary : \n",
      "in this paper, we consider the direct detection of gravitational waves ( gws ) by pulsar timing arrays ( ptas ).\n",
      "Sample Score : \n",
      "Criteria:rouge1, Score:Score(precision=0.5882352941176471, recall=0.10101010101010101, fmeasure=0.17241379310344826)\n",
      "Criteria:rouge2, Score:Score(precision=0.25, recall=0.04081632653061224, fmeasure=0.07017543859649122)\n",
      "Criteria:rougeL, Score:Score(precision=0.5294117647058824, recall=0.09090909090909091, fmeasure=0.15517241379310345)\n"
     ]
    }
   ],
   "source": [
    "content = '.'.join(arxiv_test.loc[1, 'article_text'])\n",
    "pegasusSummary = get_response(content)[0] #Getting the Response from the model\n",
    "print(\"Sample Summary : \")\n",
    "print(pegasusSummary)\n",
    "\n",
    "goldenSummary = '.'.join(arxiv_test.loc[1, 'abstract_text'])\n",
    "score = Score(trueSummary = goldenSummary, predSummary = pegasusSummary)\n",
    "print(\"Sample Score : \")\n",
    "for key, value in score.rougeScore().items():\n",
    "    print(f\"Criteria:{key}, Score:{value}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c72407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:15:07.796913: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-20 22:15:07.796961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-20 22:15:07.815075: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-20 22:15:08.806721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.75772833824158\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "results = bertscore.compute(predictions=[pegasusSummary], references=[goldenSummary], lang=\"en\")\n",
    "print(results['f1'][0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d055e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# pick only the first 1000 rows from the dataframes\n",
    "arxiv_test = arxiv_test[:10]\n",
    "# pubmed_test = pubmed_test[:100]\n",
    "\n",
    "# creating 'Gold Summary' column\n",
    "def mapping(row):\n",
    "    row['Gold Summary'] = ''.join(row['abstract_text'])\n",
    "    return row\n",
    "\n",
    "arxiv_test = arxiv_test.apply(mapping, axis=1)\n",
    "# pubmed_test = pubmed_test.apply(mapping, axis=1)\n",
    "\n",
    "# generating summaries\n",
    "def generateSummary(row):\n",
    "    article = ''.join(row['article_text'])\n",
    "    summary = get_response(article)[0]\n",
    "    row['Generated Summary'] = summary\n",
    "    print(f\"Generated summary for {row['article_id']}.\")\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ec9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.evaluation import rougeScores\n",
    "\n",
    "start_time = time.time()\n",
    "arxiv_test = arxiv_test.apply(generateSummary, axis=1)\n",
    "print('Time taken for arxiv: ', time.time() - start_time)\n",
    "\n",
    "arxiv_test, rougeScoresArxiv = rougeScores(arxiv_test)\n",
    "# pubmed_test, rougeScoresPubmed = rougeScores(pubmed_test)\n",
    "\n",
    "# printing the results\n",
    "print('arxiv')\n",
    "print('rouge1: ', np.mean([ score.fmeasure for score in rougeScoresArxiv['rouge1'] ]))\n",
    "# print('rouge2: ', np.mean([ score.fmeasure for score in rougeScoresArxiv['rouge2'] ]))\n",
    "# print('rougeL: ', np.mean([ score.fmeasure for score in rougeScoresArxiv['rougeL'] ]))\n",
    "print('BertScore : ', np.mean( [score for score in rougeScoresArxiv['bertScore']]))\n",
    "# saving the results\n",
    "arxiv_test.to_csv('arxiv_test_pegasus.csv')\n",
    "# pubmed_test.to_csv('pubmed_test_pegasus.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
