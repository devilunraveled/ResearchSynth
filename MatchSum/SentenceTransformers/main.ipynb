{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('intfloat/e5-large-v2')\n",
    "model1 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the second model to generate embeddings for the document and the summary.\n",
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "\n",
    "with open('../data/paper1', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())\n",
    "with open('../data/abstract1', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())    \n",
    "with open('../data/paper2', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())\n",
    "with open('../data/abstract2', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embeddings\n",
    "We will generate the embeddings and see the semantic similarity score for abstract and the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Paper 1:\n",
      "\tAbstract 1 Score: 0.9030022621154785, Abstract 2 Score: 0.7938213348388672.\n",
      "For Paper 2:\n",
      "\tAbstract 1 Score: 0.7524808645248413, Abstract 2 Score: 0.8678621053695679.\n",
      "For Paper 1:\n",
      "\tAbstract 1 Score: 0.8340383172035217, Abstract 2 Score: 0.13522927463054657.\n",
      "For Paper 2:\n",
      "\tAbstract 1 Score: 0.048615314066410065, Abstract 2 Score: 0.7643997669219971.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = model.encode(input_texts, normalize_embeddings=True)\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"For Paper 1:\\n\\tAbstract 1 Score: {np.dot(embeddings[0], embeddings[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings[0], embeddings[3])}.\\n\\\n",
    "For Paper 2:\\n\\tAbstract 1 Score: {np.dot(embeddings[2], embeddings[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings[2], embeddings[3])}.\")\n",
    "\n",
    "embeddings1 = model1.encode(input_texts)\n",
    "embeddings1 = np.array(embeddings1)\n",
    "print(f\"For Paper 1:\\n\\tAbstract 1 Score: {np.dot(embeddings1[0], embeddings1[1]) / np.linalg.norm(embeddings1[0]) / np.linalg.norm(embeddings1[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings1[0], embeddings1[3]) / np.linalg.norm(embeddings1[0]) / np.linalg.norm(embeddings1[3])}.\\n\\\n",
    "For Paper 2:\\n\\tAbstract 1 Score: {np.dot(embeddings1[2], embeddings1[1]) / np.linalg.norm(embeddings1[2]) / np.linalg.norm(embeddings1[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings1[2], embeddings1[3]) / np.linalg.norm(embeddings1[2]) / np.linalg.norm(embeddings1[3])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, model1 provides a much more stark difference between the similarity of abstract with its own paper and the similarity of abstract with another paper, which is the property which we desire: good summaries will be more similar to the source documents.\n",
    "\n",
    "# Generating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array n-grams where n-grams[i] = (ith n-gram, i)\n",
    "def generate_n_grams(n, sentences):\n",
    "    return [('.'.join(sentences[i:i+n]), i) for i in range(len(sentences)-n+1)]\n",
    "\n",
    "# get the sentences\n",
    "paper1 = input_texts[0]\n",
    "sentences = paper1.split(\".\") # getting the sentences\n",
    "\n",
    "# generate n-grams\n",
    "n = 2\n",
    "n_grams = generate_n_grams(n, sentences)\n",
    "\n",
    "# calculate similarity of each n-gram with the paper\n",
    "sims = []\n",
    "for gram in n_grams:\n",
    "    embeddings1 = model1.encode([gram[0], paper1], normalize_embeddings=True)\n",
    "    sims.append((np.dot(embeddings1[0], embeddings1[1]), gram[1]))\n",
    "\n",
    "# select top k sentences\n",
    "k = 10\n",
    "r = int(np.ceil(k/n))\n",
    "topr_n_grams = sorted(sims, key=lambda x: x[0], reverse=True)[:r]\n",
    "topr_n_grams = sorted(topr_n_grams, key=lambda x: x[1]) # get the sentences back in order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$i^{th}$ row of the list `ngrams` (1-indexed) contains i-grams of the sentences from the source document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ngrams = []\n",
    "# k = 4\n",
    "# for i in range(1, k+1):\n",
    "#     ngrams.append(n_grams(i))\n",
    "\n",
    "# sims = []\n",
    "# for grams in ngrams:\n",
    "#     sims.append([])\n",
    "#     for gram in grams:\n",
    "#         embeddings1 = model1.encode([gram, paper1], normalize_embeddings=True)\n",
    "#         sims[-1].append(np.dot(embeddings1[0], embeddings1[1]))\n",
    "\n",
    "# for i in range(len(sims)):\n",
    "#     ind = np.argsort(sims[i])[::-1]\n",
    "#     sims[i] = np.array(sims[i])[ind]\n",
    "#     ngrams[i] = np.array(ngrams[i])[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models . many examples of such estimators belong to the large class of regularized kernel based methods over a reproducing kernel hilbert space @xmath0 , see e.\n",
      " @xcite . in the last years many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.\n",
      " in the last years many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.g.\n",
      " @xcite for the general case and @xcite for additive models . therefore , we will here consider the case of regularized kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term @xmath1 for some @xmath2 which is a smoothness penalty but not a sparsity penalty , see e.\n",
      " such regularized kernel based methods are now often called support vector machines ( svms ) , although the notation was historically used for such methods based on the special hinge loss function and for special kernels only , we refer to @xcite . in this paper we address the open question , whether an svm with an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , if the assumption of an additive model is satisfied \n"
     ]
    }
   ],
   "source": [
    "# print the generated sumamry\n",
    "print(\"Summary:\")\n",
    "print(\".\\n\".join([n_grams[gram[1]][0] for gram in topr_n_grams]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function to generate summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchSummarizer:\n",
    "    def __init__(self, paper, n_gram_size, model=model1):\n",
    "        # get the sentences\n",
    "        self.paper = paper\n",
    "        sentences = paper.split(\".\")\n",
    "        self.sentences = sentences\n",
    "        self.model = model\n",
    "\n",
    "        # generate n-grams\n",
    "        n = n_gram_size\n",
    "        self.n_gram_size = n_gram_size\n",
    "        n_grams = self.generate_n_grams(n, sentences)\n",
    "        self.n_grams = n_grams\n",
    "\n",
    "        # calculate similarity of each n-gram with the paper\n",
    "        sims = []\n",
    "        for gram in n_grams:\n",
    "            embeddings1 = self.model.encode([gram[0], paper], normalize_embeddings=True)\n",
    "            sims.append((np.dot(embeddings1[0], embeddings1[1]), gram[1]))\n",
    "\n",
    "        self.sims = sims\n",
    "\n",
    "    def generate_n_grams(self, n, sentences): \n",
    "        '''\n",
    "        returns an array n-grams where n-grams[i] = (ith n-gram, i)\n",
    "        '''\n",
    "        return [('.'.join(sentences[i:i+n]), i) for i in range(len(sentences)-n+1)]\n",
    "    \n",
    "    def generateSummary(self, summary_size):\n",
    "        '''\n",
    "        Generates a summary of approximatly summary_size sentences\n",
    "        '''\n",
    "        # select top k sentences\n",
    "        k = summary_size\n",
    "        r = int(np.ceil(k/n))\n",
    "        topr_n_grams = sorted(self.sims, key=lambda x: x[0], reverse=True)[:r]\n",
    "        topr_n_grams = sorted(topr_n_grams, key=lambda x: x[1]) # get the sentences back in order\n",
    "\n",
    "        # generate summary\n",
    "        return \".\\n\".join([n_grams[gram[1]][0] for gram in topr_n_grams])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from scorer import Score\n",
    "\n",
    "golden_summary = \"\"\"additive models play an important role in semiparametric statistics . \n",
    "this paper gives learning rates for regularized kernel based methods for additive models . \n",
    "these learning rates compare favourably in particular in high dimensions to recent results on \n",
    "optimal learning rates for purely nonparametric regularized kernel based quantile regression \n",
    "using the gaussian radial basis function kernel , provided the assumption of an additive model \n",
    "is valid . additionally , a concrete example is presented to show that a gaussian function \n",
    "depending only on one variable lies in a reproducing kernel hilbert space generated by an \n",
    "additive gaussian kernel , but does not belong to the reproducing kernel hilbert space \n",
    "generated by the multivariate gaussian kernel of the same variance . * key words and phrases . \n",
    "* additive model , kernel , quantile regression , semiparametric , rate of convergence , \n",
    "support vector machine .\"\"\"\n",
    "\n",
    "our_summary = MatchSummarizer(paper1, 2).generateSummary(10)\n",
    "\n",
    "score = Score(trueSummary=golden_summary, predSummary=our_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.3779904306220096\n",
      "rouge2:  0.11538461538461539\n",
      "rouge3:  0.04830917874396135\n"
     ]
    }
   ],
   "source": [
    "for sc in score.rougeScore():\n",
    "    print(f'{sc}: ', score.rougeScore()[sc].fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out different values of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_sentences = [4, 8, 12, 16, 20, 24, 28, 32]\n",
    "rouge_scores = []\n",
    "for n in n_vals:\n",
    "    summarizer = MatchSummarizer(input_texts[2], n, model=model)\n",
    "    for n_sentence in n_sentences:\n",
    "        our_summary = summarizer.generateSummary(n_sentence)\n",
    "        score = Score(trueSummary=golden_summary, predSummary=our_summary)\n",
    "        rouge_scores.append([n,\n",
    "                             n_sentence,\n",
    "                             score.rougeScore()['rouge1'].fmeasure, \n",
    "                             score.rougeScore()['rouge2'].fmeasure, \n",
    "                             score.rougeScore()['rouge3'].fmeasure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "0            1            4  0.315152  0.073171  0.042945\n",
      "   n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "8            2            4  0.378641  0.147059  0.089109\n",
      "    n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "17            3            8  0.386364  0.129771  0.069231\n",
      "    n_gram_size  n_sentences   rouge1    rouge2    rouge3\n",
      "25            4            8  0.39823  0.116071  0.054054\n",
      "    n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "37            5           24  0.395415  0.132565  0.057971\n",
      "    n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "47            6           32  0.393162  0.131805  0.057637\n",
      "    n_gram_size  n_sentences   rouge1    rouge2    rouge3\n",
      "53            7           24  0.38835  0.123779  0.059016\n",
      "    n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "60            8           20  0.430279  0.144578  0.072874\n",
      "    n_gram_size  n_sentences   rouge1    rouge2    rouge3\n",
      "70            9           28  0.38835  0.123779  0.059016\n",
      "    n_gram_size  n_sentences   rouge1    rouge2    rouge3\n",
      "79           10           32  0.38835  0.123779  0.059016\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "n_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_sentences = [4, 8, 12, 16, 20, 24]\n",
    "rouge_scores = pd.DataFrame(rouge_scores, columns=['n_gram_size', 'n_sentences', 'rouge1', 'rouge2', 'rouge3'])\n",
    "\n",
    "# plot rouge1, rouge2, rouge3 vs n_sentences for each n_gram_size\n",
    "# for n in n_vals:\n",
    "#     plt.plot(n_sentences, rouge_scores[rouge_scores['n_gram_size'] == n]['rouge1'], label=f'rouge1, n={n}')\n",
    "#     # plt.plot(n_sentences, rouge_scores[rouge_scores['n_gram_size'] == n]['rouge2'], label=f'rouge2, n={n}')\n",
    "#     # plt.plot(n_sentences, rouge_scores[rouge_scores['n_gram_size'] == n]['rouge3'], label=f'rouge3, n={n}')    \n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# best rouge1 scores for which parameters\n",
    "for n in n_vals:\n",
    "    print(rouge_scores[rouge_scores['n_gram_size'] == n].sort_values(by='rouge1', ascending=False).head(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
