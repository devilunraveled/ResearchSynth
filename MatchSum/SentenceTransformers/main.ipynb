{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('intfloat/e5-large-v2')\n",
    "model1 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "model2 = SentenceTransformer('ArtifactAI/arxiv-distilbert-base-v3-GenQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the second model to generate embeddings for the document and the summary.\n",
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "\n",
    "with open('../data/paper1', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())\n",
    "with open('../data/abstract1', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())    \n",
    "with open('../data/paper2', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())\n",
    "with open('../data/abstract2', 'r') as f:\n",
    "    input_texts.append(\"passage: \" + f.read())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embeddings\n",
    "We will generate the embeddings and see the semantic similarity score for abstract and the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Paper 1:\n",
      "\tAbstract 1 Score: 0.9030022621154785, Abstract 2 Score: 0.7938213348388672.\n",
      "For Paper 2:\n",
      "\tAbstract 1 Score: 0.7524808645248413, Abstract 2 Score: 0.8678621053695679.\n",
      "For Paper 1:\n",
      "\tAbstract 1 Score: 0.8340383172035217, Abstract 2 Score: 0.13522927463054657.\n",
      "For Paper 2:\n",
      "\tAbstract 1 Score: 0.048615314066410065, Abstract 2 Score: 0.7643997669219971.\n",
      "For Paper 1:\n",
      "\tAbstract 1 Score: 0.6787219047546387, Abstract 2 Score: 0.15100020170211792.\n",
      "For Paper 2:\n",
      "\tAbstract 1 Score: -0.025890199467539787, Abstract 2 Score: 0.7579803466796875.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = model.encode(input_texts, normalize_embeddings=True)\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"For Paper 1:\\n\\tAbstract 1 Score: {np.dot(embeddings[0], embeddings[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings[0], embeddings[3])}.\\n\\\n",
    "For Paper 2:\\n\\tAbstract 1 Score: {np.dot(embeddings[2], embeddings[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings[2], embeddings[3])}.\")\n",
    "\n",
    "embeddings1 = model1.encode(input_texts)\n",
    "embeddings1 = np.array(embeddings1)\n",
    "print(f\"For Paper 1:\\n\\tAbstract 1 Score: {np.dot(embeddings1[0], embeddings1[1]) / np.linalg.norm(embeddings1[0]) / np.linalg.norm(embeddings1[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings1[0], embeddings1[3]) / np.linalg.norm(embeddings1[0]) / np.linalg.norm(embeddings1[3])}.\\n\\\n",
    "For Paper 2:\\n\\tAbstract 1 Score: {np.dot(embeddings1[2], embeddings1[1]) / np.linalg.norm(embeddings1[2]) / np.linalg.norm(embeddings1[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings1[2], embeddings1[3]) / np.linalg.norm(embeddings1[2]) / np.linalg.norm(embeddings1[3])}.\")\n",
    "\n",
    "embeddings1 = model2.encode(input_texts)\n",
    "embeddings1 = np.array(embeddings1)\n",
    "print(f\"For Paper 1:\\n\\tAbstract 1 Score: {np.dot(embeddings1[0], embeddings1[1]) / np.linalg.norm(embeddings1[0]) / np.linalg.norm(embeddings1[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings1[0], embeddings1[3]) / np.linalg.norm(embeddings1[0]) / np.linalg.norm(embeddings1[3])}.\\n\\\n",
    "For Paper 2:\\n\\tAbstract 1 Score: {np.dot(embeddings1[2], embeddings1[1]) / np.linalg.norm(embeddings1[2]) / np.linalg.norm(embeddings1[1])}, \\\n",
    "Abstract 2 Score: {np.dot(embeddings1[2], embeddings1[3]) / np.linalg.norm(embeddings1[2]) / np.linalg.norm(embeddings1[3])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, model1 provides a much more stark difference between the similarity of abstract with its own paper and the similarity of abstract with another paper, which is the property which we desire: good summaries will be more similar to the source documents.\n",
    "\n",
    "# Generating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array n-grams where n-grams[i] = (ith n-gram, i)\n",
    "def generate_n_grams(n, sentences):\n",
    "    return [('.'.join(sentences[i:i+n]), i) for i in range(len(sentences)-n+1)]\n",
    "\n",
    "# get the sentences\n",
    "paper1 = input_texts[0]\n",
    "sentences = paper1.split(\".\") # getting the sentences\n",
    "\n",
    "# generate n-grams\n",
    "n = 2\n",
    "n_grams = generate_n_grams(n, sentences)\n",
    "\n",
    "# calculate similarity of each n-gram with the paper\n",
    "sims = []\n",
    "for gram in n_grams:\n",
    "    embeddings1 = model2.encode([gram[0], paper1], normalize_embeddings=True)\n",
    "    sims.append((np.dot(embeddings1[0], embeddings1[1]), gram[1]))\n",
    "\n",
    "# select top k sentences\n",
    "k = 10\n",
    "r = int(np.ceil(k/n))\n",
    "topr_n_grams = sorted(sims, key=lambda x: x[0], reverse=True)[:r]\n",
    "topr_n_grams = sorted(topr_n_grams, key=lambda x: x[1]) # get the sentences back in order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$i^{th}$ row of the list `ngrams` (1-indexed) contains i-grams of the sentences from the source document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ngrams = []\n",
    "# k = 4\n",
    "# for i in range(1, k+1):\n",
    "#     ngrams.append(n_grams(i))\n",
    "\n",
    "# sims = []\n",
    "# for grams in ngrams:\n",
    "#     sims.append([])\n",
    "#     for gram in grams:\n",
    "#         embeddings1 = model1.encode([gram, paper1], normalize_embeddings=True)\n",
    "#         sims[-1].append(np.dot(embeddings1[0], embeddings1[1]))\n",
    "\n",
    "# for i in range(len(sims)):\n",
    "#     ind = np.argsort(sims[i])[::-1]\n",
    "#     sims[i] = np.array(sims[i])[ind]\n",
    "#     ngrams[i] = np.array(ngrams[i])[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " @xcite . in the last years many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.\n",
      " in the last years many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.g.\n",
      " @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and the references therein . of course , the least squares loss function is differentiable and has many nice mathematical properties , but it is only locally lipschitz continuous and therefore regularized kernel based methods based on this loss function typically suffer on bad statistical robustness properties , even if the kernel is bounded .\n",
      " of course , the least squares loss function is differentiable and has many nice mathematical properties , but it is only locally lipschitz continuous and therefore regularized kernel based methods based on this loss function typically suffer on bad statistical robustness properties , even if the kernel is bounded . this is in sharp contrast to kernel methods based on a lipschitz continuous loss function and on a bounded loss function , where results on upper bounds for the maxbias bias and on a bounded influence function are known , see e.\n",
      " @xcite for the general case and @xcite for additive models . therefore , we will here consider the case of regularized kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term @xmath1 for some @xmath2 which is a smoothness penalty but not a sparsity penalty , see e\n"
     ]
    }
   ],
   "source": [
    "# print the generated sumamry\n",
    "print(\"Summary:\")\n",
    "print(\".\\n\".join([n_grams[gram[1]][0] for gram in topr_n_grams]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function to generate summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BaseSummarizer:\n",
    "    def __init__(self, paper, n_gram_size, model=None):\n",
    "        '''\n",
    "        paper: the research paper to summarize as a string.\n",
    "        n_gram_size: the size of the n-grams to use for summarization.\n",
    "        model: the model that will return the embedding for a given text.\n",
    "               should have a method `encode` that takes a list of strings \n",
    "               and returns a list of embeddings.\n",
    "        '''\n",
    "        if model is None:\n",
    "            raise ValueError(\"model cannot be None.\")\n",
    "\n",
    "        # get the sentences\n",
    "        self.paper = paper\n",
    "        sentences = paper.split(\".\")\n",
    "        self.sentences = sentences\n",
    "        self.model = model\n",
    "\n",
    "        # generate n-grams\n",
    "        n = n_gram_size\n",
    "        self.n_gram_size = n_gram_size\n",
    "        n_grams = self.generate_n_grams(n, sentences)\n",
    "        self.n_grams = n_grams\n",
    "\n",
    "        # calculate similarity of each n-gram with the paper\n",
    "        sims = []\n",
    "        for gram in n_grams:\n",
    "            embeddings1 = self.model.encode([gram[0], paper], normalize_embeddings=True)\n",
    "            sims.append((np.dot(embeddings1[0], embeddings1[1]), gram[1]))\n",
    "\n",
    "        self.sims = sims\n",
    "\n",
    "    def generate_n_grams(self, n, sentences): \n",
    "        '''\n",
    "        returns an array n-grams where n-grams[i] = (ith n-gram, i)\n",
    "        '''\n",
    "        return [('.'.join(sentences[i:i+n]), i) for i in range(len(sentences)-n+1)]\n",
    "\n",
    "    def generateSummary(self, summary_size):\n",
    "        # select top k sentences\n",
    "        k = summary_size\n",
    "        sorted_n_grams = sorted(self.sims, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        n_gram_ids = []\n",
    "        done_set = set()\n",
    "\n",
    "        # select top k sentences\n",
    "        for sim, index in sorted_n_grams:\n",
    "            if index in done_set:\n",
    "                continue\n",
    "            for x in range(index-self.n_gram_size+1, index+self.n_gram_size):\n",
    "                done_set.add(x)\n",
    "\n",
    "            n_gram_ids.append(index)\n",
    "            k -= self.n_gram_size\n",
    "            if k <= 0 or k < self.n_gram_size/2:\n",
    "                break\n",
    "\n",
    "        # generate summary\n",
    "        return \".\\n\".join([self.n_grams[id][0] for id in n_gram_ids])\n",
    "    \n",
    "class MatchSummarizer():\n",
    "    def __init__(self, paper, n_gram_size, model=None, cmp_model=None):\n",
    "        '''\n",
    "        paper: the research paper to summarize as a string.\n",
    "        n_gram_size: the size of the n-grams to use for summarization.\n",
    "        model: the model that will return the embedding for a given text.\n",
    "               should have a method `encode` that takes a list of strings \n",
    "               and returns a list of embeddings.\n",
    "        '''\n",
    "        if model is None:\n",
    "            raise ValueError(\"model cannot be None.\")\n",
    "        if cmp_model is None:\n",
    "            raise ValueError(\"cmp_model cannot be None.\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.cmp_model = cmp_model\n",
    "        self.paper = paper\n",
    "        self.n_gram_size = n_gram_size\n",
    "\n",
    "    def generateSummary(self):\n",
    "        n_vals = [2, 3, 4, 5, 8]\n",
    "        n_sentences = [8, 10, 12]\n",
    "\n",
    "        best_summary = \"\"\n",
    "        best_sim = -1\n",
    "        for n in n_vals:\n",
    "            summarizer = BaseSummarizer(self.paper, n, self.model)\n",
    "            for n_sentence in n_sentences:\n",
    "                summary = summarizer.generateSummary(n_sentence)\n",
    "                embeddings = self.cmp_model.encode([summary, self.paper], normalize_embeddings=True)\n",
    "                sim = np.dot(embeddings[0], embeddings[1])\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_summary = summary\n",
    "        return best_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from Scorer import Score\n",
    "\n",
    "golden_summary = \"\"\"additive models play an important role in semiparametric statistics . \n",
    "this paper gives learning rates for regularized kernel based methods for additive models . \n",
    "these learning rates compare favourably in particular in high dimensions to recent results on \n",
    "optimal learning rates for purely nonparametric regularized kernel based quantile regression \n",
    "using the gaussian radial basis function kernel , provided the assumption of an additive model \n",
    "is valid . additionally , a concrete example is presented to show that a gaussian function \n",
    "depending only on one variable lies in a reproducing kernel hilbert space generated by an \n",
    "additive gaussian kernel , but does not belong to the reproducing kernel hilbert space \n",
    "generated by the multivariate gaussian kernel of the same variance . * key words and phrases . \n",
    "* additive model , kernel , quantile regression , semiparametric , rate of convergence , \n",
    "support vector machine.\"\"\"\n",
    "\n",
    "our_summary = MatchSummarizer(paper1, 2, model2, model1).generateSummary()\n",
    "\n",
    "score = Score(trueSummary=golden_summary, predSummary=our_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:  0.38743455497382195\n",
      "rouge2:  0.10526315789473684\n",
      "rougeL:  0.17277486910994763\n"
     ]
    }
   ],
   "source": [
    "for sc in score.rougeScore():\n",
    "    print(f'{sc}: ', score.rougeScore()[sc].fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out different values of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summarizer \u001b[39m=\u001b[39m MatchSummarizer(input_texts[\u001b[39m0\u001b[39m], n, model2, model2)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m our_summary \u001b[39m=\u001b[39m summarizer\u001b[39m.\u001b[39;49mgenerateSummary()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m score \u001b[39m=\u001b[39m Score(trueSummary\u001b[39m=\u001b[39mgolden_summary, predSummary\u001b[39m=\u001b[39mour_summary)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(score\u001b[39m.\u001b[39mrougeScore())\n",
      "\u001b[1;32m/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb Cell 18\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m best_sim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m n_vals:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     summarizer \u001b[39m=\u001b[39m BaseSummarizer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpaper, n, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     \u001b[39mfor\u001b[39;00m n_sentence \u001b[39min\u001b[39;00m n_sentences:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m         summary \u001b[39m=\u001b[39m summarizer\u001b[39m.\u001b[39mgenerateSummary(n_sentence)\n",
      "\u001b[1;32m/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m sims \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m gram \u001b[39min\u001b[39;00m n_grams:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     embeddings1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode([gram[\u001b[39m0\u001b[39;49m], paper], normalize_embeddings\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     sims\u001b[39m.\u001b[39mappend((np\u001b[39m.\u001b[39mdot(embeddings1[\u001b[39m0\u001b[39m], embeddings1[\u001b[39m1\u001b[39m]), gram[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ashmitchamoli/Desktop/Projects/IRE_Project/ResearchSynth/MatchSum/SentenceTransformers/main.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msims \u001b[39m=\u001b[39m sims\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:161\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m--> 161\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(sentences_batch)\n\u001b[1;32m    162\u001b[0m     features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:319\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, texts: Union[List[\u001b[39mstr\u001b[39m], List[Dict], List[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]]):\n\u001b[1;32m    316\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m    Tokenizes the texts\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_first_module()\u001b[39m.\u001b[39;49mtokenize(texts)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:113\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case:\n\u001b[1;32m    111\u001b[0m     to_tokenize \u001b[39m=\u001b[39m [[s\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m to_tokenize]\n\u001b[0;32m--> 113\u001b[0m output\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\u001b[39m*\u001b[39;49mto_tokenize, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongest_first\u001b[39;49m\u001b[39m'\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_seq_length))\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2806\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2804\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2805\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2806\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2807\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2808\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2892\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2887\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2888\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2889\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2890\u001b[0m         )\n\u001b[1;32m   2891\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2893\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2894\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2895\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2896\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2897\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2898\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2899\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2900\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2901\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2902\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2903\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2904\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2905\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2906\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2907\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2908\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2909\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2910\u001b[0m     )\n\u001b[1;32m   2911\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2912\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2913\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2914\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2930\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2931\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3083\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3074\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3075\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3076\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3080\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3081\u001b[0m )\n\u001b[0;32m-> 3083\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3084\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3085\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3086\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3087\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3088\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3089\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3090\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3091\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3092\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3093\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3094\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3095\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3096\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3097\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3098\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3099\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3100\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3101\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:472\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    465\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    466\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    470\u001b[0m )\n\u001b[0;32m--> 472\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    473\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    474\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    475\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    476\u001b[0m )\n\u001b[1;32m    478\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    484\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    485\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    486\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    496\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summarizer = MatchSummarizer(input_texts[0], n, model2, model2)\n",
    "our_summary = summarizer.generateSummary()\n",
    "score = Score(trueSummary=golden_summary, predSummary=our_summary)\n",
    "print(score.rougeScore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "0  all-mpnet-base-v2            1            8  0.367615  0.149451  0.196937\n",
      "          model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "3  all-mpnet-base-v2            2            8  0.445055  0.171271  0.203297\n",
      "          model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "7  all-mpnet-base-v2            3           12  0.429708  0.165333  0.196286\n",
      "          model_name  n_gram_size  n_sentences   rouge1  rouge2    rougeL\n",
      "9  all-mpnet-base-v2            4            8  0.42236   0.125  0.192547\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "12  all-mpnet-base-v2            5            8  0.391185  0.110803  0.176309\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "16  all-mpnet-base-v2            6           12  0.389041  0.110193  0.175342\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "18  all-mpnet-base-v2            7            8  0.456929  0.150943  0.202247\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "21  all-mpnet-base-v2            8            8  0.455224  0.150376  0.201493\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "24  all-mpnet-base-v2            9            8  0.438849  0.144928  0.194245\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rougeL\n",
      "27  all-mpnet-base-v2           10            8  0.419753  0.124224  0.185185\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "n_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_sentences = [8, 12, 16]\n",
    "rouge_scores = pd.DataFrame(rouge_scores, columns=['model_name', 'n_gram_size', 'n_sentences', 'rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "# plot rouge1, rouge2, rouge3 vs n_sentences for each n_gram_size\n",
    "# for n in n_vals:\n",
    "#     plt.plot(n_sentences, rouge_scores[rouge_scores['n_gram_size'] == n]['rouge1'], label=f'rouge1, n={n}')\n",
    "#     # plt.plot(n_sentences, rouge_scores[rouge_scores['n_gram_size'] == n]['rouge2'], label=f'rouge2, n={n}')\n",
    "#     # plt.plot(n_sentences, rouge_scores[rouge_scores['n_gram_size'] == n]['rouge3'], label=f'rouge3, n={n}')    \n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# best rouge1 scores for which parameters\n",
    "for n in n_vals:\n",
    "    print(rouge_scores[rouge_scores['n_gram_size'] == n].sort_values(by='rouge1', ascending=False).head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_sentences = [8, 12, 16]\n",
    "rouge_scores = []\n",
    "\n",
    "models = ['all-mpnet-base-v2']\n",
    "for n in n_vals:\n",
    "    for model in models:\n",
    "        if model == 'all-mpnet-base-v2':\n",
    "            sim_model = model1\n",
    "        else:\n",
    "            sim_model = model\n",
    "        summarizer = MatchSummarizer(input_texts[0], n, model=sim_model, model2)\n",
    "        for n_sentence in n_sentences:\n",
    "            our_summary = summarizer.generateSummary(n_sentence)\n",
    "            score = Score(trueSummary=golden_summary, predSummary=our_summary)\n",
    "            rouge_scores.append([model, \n",
    "                                 n,\n",
    "                                 n_sentence,\n",
    "                                 score.rougeScore()['rouge1'].fmeasure, \n",
    "                                 score.rougeScore()['rouge2'].fmeasure, \n",
    "                                 score.rougeScore()['rougeL'].fmeasure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "0  all-mpnet-base-v2            1            8  0.367615  0.149451  0.196937\n",
      "          model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "3  all-mpnet-base-v2            2            8  0.445055  0.171271  0.203297\n",
      "          model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "7  all-mpnet-base-v2            3           12  0.429708  0.165333  0.196286\n",
      "          model_name  n_gram_size  n_sentences   rouge1  rouge2    rouge3\n",
      "9  all-mpnet-base-v2            4            8  0.42236   0.125  0.192547\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "12  all-mpnet-base-v2            5            8  0.391185  0.110803  0.176309\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "16  all-mpnet-base-v2            6           12  0.389041  0.110193  0.175342\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "18  all-mpnet-base-v2            7            8  0.456929  0.150943  0.202247\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "21  all-mpnet-base-v2            8            8  0.455224  0.150376  0.201493\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "24  all-mpnet-base-v2            9            8  0.438849  0.144928  0.194245\n",
      "           model_name  n_gram_size  n_sentences    rouge1    rouge2    rouge3\n",
      "27  all-mpnet-base-v2           10            8  0.419753  0.124224  0.185185\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "n_vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_sentences = [8, 12, 16]\n",
    "rouge_scores = pd.DataFrame(rouge_scores, columns=['model_name', 'n_gram_size', 'n_sentences', 'rouge1', 'rouge2', 'rouge3'])\n",
    "\n",
    "# best rouge1 scores for which parameters\n",
    "for n in n_vals:\n",
    "    print(rouge_scores[rouge_scores['n_gram_size'] == n].sort_values(by='rouge1', ascending=False).head(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
